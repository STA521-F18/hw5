---
title: 'HW5: Team [my team #/name here]'
author: '[My team member names here]'
date: " "
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---


```{r setup, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(BAS)
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages

```


We have seen that as the number of features in a model increases, the training error will necessarily decrease, but the test error may not.  
For this assignment we will explore this using simulation of data to compare methods for estimation and model selection.

Some "guideposts" for when to finish parts are provided within the problem set.

1.  Generate a dataset with $p = 20$ features and $n=1000$ as follows:
First let's set our random seed in case we need to rerun parts later.

```{r jenny, echo=TRUE}
# set the random seed so that we can replicate results.
set.seed(8675309)
```

In order to simulate data, we need to specify the values of the  "true" parameters.  For this study we will use

```{r true}
# true parameters
sigma = 2.5
betatrue = c(1,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1.5,3.5)
#          int|    X1                            | X2     |X3 

truemodel = betatrue != 0
```



Generate Data with correlated columns.
```{r data, cache=TRUE} 

#sample size
n = 1000

# generate some standard normals
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  
#  Create X1 by taking linear cominations of Z to induce correlation among X1 components
  
  X1 = cbind(Z, 
             (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n))
             )
# generate X2 as a standard normal  
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  
# Generate X3 as a linear combination of X2 and noise  
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  
# combine them  
  X <- cbind(X1,X2,X3)

# Generate mu     
# X does not have a column of ones for the intercept so need to add the intercept  
# for true mu  
mu = betatrue[1] + X %*% betatrue[-1] 
  
# now generate Y  
Y = mu + rnorm(n,0,sigma)  
  
# make a dataframe and save it
df = data.frame(Y, X, mu)
```






2. Split your data set into a training set containing $200$ observations and a test set containing $700$ observations.  Before splitting reset the random seed based on your team number
```{r new-seed}
set.seed(0)   # replace 0 with team number before runing
```





3.  Using Ordinary Least squares based on fitting the full model for the training data,  compute the average RMSE for a) estimating $\beta_{true}$, b) estimating $\mu_{true} = X_{\text{test}} \beta_{true}$ and c) out of sample prediction of $Y_{test}$ for the test data.
Note for a vector of length $d$, RMSE is defined as
$$
RMSE(\hat{\theta}) = \sqrt{\sum_{i = 1}^{d} (\hat{\theta}_j - \theta_j)^2/d}
$$
Provide Confidence/prediction intervals for $\beta$, and $\mu$, and $Y$ in the test data and report what percent of the intervals contain the true values.  Do any estimates seem surprising?


3. Perform best subset selection on the training data, and plot the training set RMSE for fitting associated with the best model of each size.

4. Plot the test set RMSE for prediction associated with the best model of each size.   For which model size does the test set RMSE take on its minimum value?  Comment on your results.  If it takes on its minimum value for a model withonly an intercept or a model containing all of the predictors, adjust $\sigma$ used in generating the data until the test set RMSE is minimized for an intermediate point.

5.  How does the model at which the test set RMSE is minimized compare to the true model used to generate the data?  Comment on the coefficient values and confidence intervals obtained from using all of the data.  Do the intervals include the true values?

6.  Use AIC with stepwise or all possible subsets to select a model based on the training data and then use OLS to estimate the parameters under that model.  Using the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) estimating $\mu_{true}$ in the test data, and c) predicting $Y_{test}$. For prediction, does this find the best model in terms of RMSE? Does AIC find the true model?  Comment on your findings.



7.   Use BIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Use the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) $\mu_{true}$ for the test data, and c) predicting $Y_{test}$.   For prediction, does this find the best model in terms of RMSE? Does BIC find the true model?  Comment on your findings.

8.  Take a look at the summaries from the estimates under the best AIC and BIC models fit to the training data. Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.  (Provide a graph) 

10. Provide a paragraph  summarizing your findings and any recommendations for model selection and inference for the tasks of prediction of future data, estimation of parameters or selecting the true model.



11. This problem uses the `Caravan` data from the package `ISLR`. To start reset your random seed based on your group number.  Divide the data into into a training and test data set.   _Suggest working together as a team and using `cache=T` on (b) and (c) given the size of the problem_

      a)  Fit a logistic regression model to the training data with all predictors to predict if an individual will purchase insurance and provide a table of estimates with confidence intervals and any relevant summaries of the model.  Comment on your findings - are there any concerns?
    
      
     b)  Use best subsets regression with AIC to find the best subset of predictors using the training data.   _Note:  do not show all ouput from step, but just the final model!_
     
     c) Create  a table of estimates with confidence intervals any relevant summaries of the final model. Comment on the findings - which predictors are most important? 
      
       d)  Use best subsets regression with BIC to find the best subset of predictors using the training data.    If you start BIC at the best AIC model found using step will this result in the same model as if you started at the full model?  _Note:  do not show all ouput from step!_ 
       
       e) Create  a table of estimates with confidence intervals any relevant summaries of the final model. Which predictors are most important?  Are all of the variables included here also included in the best AIC model?   Are there any that include zero in the credible (confidence) interval? 
       
      f)  For each method, create a  confusion matrix for the classifications on the observed data,  a table where the diagonal elements show how many times the model predicts correctly and the off-diagonal elements indicate miss-classification and report the missclasification rate for each model.  For creating the predictions, use a cut-off of 0.2.
     
```{r}
table(train$Purchase, predict > .2)
```
     

    g)  Create confusion matrices using the test data.   Which model has the lowest missclassification rate out of sample?  Which error do you think is worse in this context from the company's perspective:  predicting that someone will buy insurance and they don't or  predicting that someone will buy not buy insurance and they do?
    
    
    h)  For each method, create an ROC curve  and estimate of AUC (see lab) using the training and test data.  Which method has the best AUC?  Does the method with the best AUC in the training data have the best AUC in the test data?   
    
    
    i)  Provide a half-page (max) report  briefly describing what you did and your findings.  For the five most important variables in your final model report interval estimates of the odds ratios and interpretations in the context of the problem.  
    





